{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dpkt\n",
    "#import matplotlib.pyplot as plt\n",
    "#import numpy\n",
    "#import math\n",
    "# import pickle5 as pickle\n",
    "import pickle # für Obj serialisation\n",
    "#import numpy as np\n",
    "#import matplotlib\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "#import csv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransitionPeriod = 4\n",
    "amazon_ips = {} # Amazon meint wohl ob eingehend oder ausgehend\n",
    "GAMMA = 0.25\n",
    "observation_lengths = [180, 300, 900, 1800] # in seconds (3min - 30min)\n",
    "LENGTH = 3600 #\n",
    "\"\"\"Eq 3600\"\"\"\n",
    "MAGIC_IsUserPacketARelevantBurstPacket = 40 # Wird bei der Event Extraction verwendet... Wenn größer, dann wird Paket als Relevantes Burst Paket verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA = 15 # seconds -> Maximale Zeitliche Differenz.. Wird schrittweise erhöht \n",
    "size_threshold = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter aus Paper:\n",
    "- t_e = 0.5s (Burst detection Threshold) \n",
    "- Delta = 3s (Abweichung der Event Times)\n",
    "- Γ (Gamma) = 10kb (Abweichung der BurstGröße)\n",
    "\n",
    " \n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reads files and the Preprocessed \n",
    "\n",
    "def get_message_file_names(rangeBegin, rangeEnd, messages_dir):\n",
    "    \"\"\"Copys All messages contained in \\\"all_messages_path\\\" to arg \\\" messages_dir\\\"\"\"\"\n",
    "\n",
    "    all_messages_path = 'path/to/generated_messages/messages_pareto_xm_5000_alpha_0.93_max30delay_ver02' # Message Traces ?\n",
    "    all_messages_file_names = sorted(os.listdir(all_messages_path))\n",
    "    if not os.path.exists(messages_dir):\n",
    "        os.makedirs(messages_dir)\n",
    "    for i in range(len(all_messages_file_names)):\n",
    "        if i >= rangeBegin and i <= rangeEnd:\n",
    "            print(all_messages_file_names[i])\n",
    "            os.system('cp {} {}'.format(os.path.join(all_messages_path, all_messages_file_names[i]), os.path.join(messages_dir,all_messages_file_names[i])))\n",
    "    return\n",
    "\n",
    "\n",
    "root_dir = 'path/to/root/directory/of/traces' # Root folder of Traces\n",
    "\n",
    "\n",
    "timestamps_dirs = []\n",
    "messages_dirs = [] # Generated Message Traces -> Concrete Sub folder\n",
    "message_file_names = []\n",
    "packets_pickle_dirs = []\n",
    "packets = []\n",
    "dir_count = 0\n",
    "for path in sorted(Path(root_dir).iterdir()): # for every file in RootDir\n",
    "\n",
    "    if path.is_dir() and path.name != 'aggregate' and not path.name.endswith('extra'): # if is Directory        \n",
    "        print(path)\n",
    "\n",
    "        timestamps_dirs.append(os.path.join(path, 'timestamps')) # fügt Timestamp Paths zu Timestamp Ordner hinzu\n",
    "        print('timestamps are in {}'.format(timestamps_dirs[-1]))\n",
    "\n",
    "        messages_dir = Path(os.path.join(path, 'messages')) # fügt dem Message ordner die konkreten Pfad hinzu\n",
    "        messages_dir.mkdir(parents=True, exist_ok=True) # kp wieso dafür ein neuer ordner erstellt wird ?\n",
    "        messages_dirs.append(os.path.join(path, 'messages')) # fügt ein neuen message pfad dem messages_dirs hinzu        \n",
    "        print('messages are in {}'.format(messages_dirs[-1]))\n",
    "\n",
    "        range_begin = int(path.name.split('_')[0])\n",
    "        range_end = int(path.name.split('_')[1])\n",
    "        print(range_begin, range_end)\n",
    "\n",
    "        get_message_file_names(range_begin, range_end, messages_dirs[dir_count])\n",
    "        message_file_names.extend(sorted(os.listdir(messages_dirs[dir_count])))\n",
    "        print('length of message_file_names is {}'.format(len(message_file_names)))\n",
    "\n",
    "        packets_pickle_dirs.append(os.path.join(path, 'traces/pickles/packets'))\n",
    "\n",
    "\n",
    "        # Reading in preprocessed Packets (serialized with JSON)\n",
    "        # read all packets from packets pickle and write them to 'packets'\n",
    "        pickle_name = os.listdir(packets_pickle_dirs[dir_count])[0]\n",
    "        print('loading {} ...'.format(pickle_name))\n",
    "        start = time.time()\n",
    "\n",
    "        with open(os.path.join(packets_pickle_dirs[dir_count], pickle_name[:-7] + '.pickle'), 'rb') as handle: #Ist wie try catch with Ressource (handle)\n",
    "            packets.extend(pickle.load(handle))\n",
    "\n",
    "        end = time.time()\n",
    "        hours, rem = divmod(end - start, 360)\n",
    "        mins, seconds = divmod(rem, 60)\n",
    "\n",
    "        print('Pickle loading took {:0>2}:{:0>2}:{:05.2f}'.format(int(hours), int (mins), seconds))\n",
    "\n",
    "        dir_count += 1\n",
    "aggregate_dir = Path(root_dir) / 'aggregate'\n",
    "aggregate_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir = str(aggregate_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps_of_all_senders = []\n",
    "\"\"\"\n",
    "### Array of timestamps array (grouped by files?)\n",
    "\"\"\"\n",
    "message_sizes_of_all_senders = []\n",
    "message_types_of_all_senders = []\n",
    "for i in range(dir_count):\n",
    "    timestamps_of_all_senders.extend(get_timestamps_of_all_senders(messages_dirs[i], timestamps_dirs[i]))\n",
    "    message_sizes_of_all_senders.extend(get_sizes_of_all_senders(messages_dirs[i]))\n",
    "    message_types_of_all_senders.extend(get_types_of_all_senders(messages_dirs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'to_amazon': False,\n",
       " 'src': '192.168.122.91',\n",
       " 'dst': '192.168.122.75',\n",
       " 'size': 64,\n",
       " 'timestamp': 1634137925.66455,\n",
       " 'protocol': 6,\n",
       " 's_port': 1080,\n",
       " 'd_port': 40940}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packets[500] # Should generate IllegalAcces.. (Should not be set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sizes_sender(messages_file, max_count=None):\n",
    "    \"\"\"\n",
    "    Returns the Sizes (as Array) from Message Traces JSON\n",
    "    -> Loads Message File (JSON)\n",
    "    \"\"\"\n",
    "    sizes = []\n",
    "    with open(messages_file) as f:\n",
    "        data = json.load(f) # die Generated Message Traces\n",
    "        count = 0\n",
    "        for message in data['messages']:\n",
    "            if max_count is None or count < max_count:\n",
    "                sizes.append(int(message['size'])) # sizes are in bytes\n",
    "            else:\n",
    "                break\n",
    "            count += 1\n",
    "    return sizes\n",
    "    \n",
    "def get_timestamps_sender(timestamps_file, max_count=None):\n",
    "    \"\"\"\n",
    "    Returns an Array of Timestamps from the Timestamps file (list of Tmstmps an IDs)\n",
    "    \"\"\"\n",
    "    timestamps = []\n",
    "    with open(timestamps_file) as f:\n",
    "        lines = f.readlines()\n",
    "        count = 0\n",
    "        for line in lines:\n",
    "            if max_count is None or count < max_count:\n",
    "                line = line.split()\n",
    "                timestamps.append((float)(line[2])) # Nimmt den Timestamp (spalte 3)\n",
    "            else:\n",
    "                break\n",
    "            count += 1\n",
    "    return timestamps\n",
    "\n",
    "def get_sizes_of_all_senders(messages_dir):\n",
    "    \"\"\"\n",
    "    Returns the Sizes as Array from all Message traces Files in messagesdir\n",
    "    \"\"\"\n",
    "    message_file_names = sorted(os.listdir(messages_dir))\n",
    "    all_message_sizes_of_senders = []\n",
    "    for mf_name in message_file_names:\n",
    "        message_sizes = get_sizes_sender(os.path.join(messages_dir, mf_name))\n",
    "        all_message_sizes_of_senders.append(message_sizes)\n",
    "    return all_message_sizes_of_senders\n",
    "\n",
    "def get_timestamps_of_all_senders(messages_dir, timestamps_root_dir):\n",
    "    \"\"\"\n",
    "    Returns all corresponding timestamps of messages_traces... (it takes the timestamp file corresponding to the Messages file from messages_dir)\n",
    "    As Array of Timestamps Array\n",
    "    \"\"\"\n",
    "    message_file_names = sorted(os.listdir(messages_dir))\n",
    "    all_timestamps = []\n",
    "    for mf_name in message_file_names:\n",
    "        timestamps = get_timestamps_sender(os.path.join(timestamps_root_dir, 'timestamps_' + mf_name[:-5] + '.txt'))\n",
    "        all_timestamps.append(timestamps)\n",
    "    return all_timestamps\n",
    "\n",
    "def get_types_of_all_senders(messages_dir, max_count=None):\n",
    "    \"\"\"\n",
    "    Return all Types as Array from all Messages in Messages dir (goes through all files and every message in each file)\n",
    "    \"\"\"\n",
    "    message_file_names = sorted(os.listdir(messages_dir))\n",
    "    all_types = []\n",
    "    for messages_file in message_file_names:\n",
    "        with open(os.path.join(messages_dir, messages_file)) as f:\n",
    "            types = []\n",
    "            data = json.load(f)\n",
    "            count = 0\n",
    "            for message in data['messages']:\n",
    "                if max_count is None or count < max_count:\n",
    "                    types.append((message['type']))\n",
    "                else:\n",
    "                    break\n",
    "                count += 1\n",
    "            all_types.append(types)\n",
    "    return all_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_receiver_packets_non_chronological_traces(packets, timestamps_of_all_senders):\n",
    "    \"\"\"\n",
    "    Returns a Array containing Arrays of Packets grouped by Timestamp files (with padding)\n",
    "    \"\"\"\n",
    "    packets_of_all_receivers = []\n",
    "    all_beginnings = []\n",
    "    all_endings = []\n",
    "    for sender_timestamps in timestamps_of_all_senders:\n",
    "        \"\"\"\n",
    "        Erstellt Array von Allen Start und Ends der Timestamp files (mit puffer)\n",
    "        \"\"\"\n",
    "        beginning = sender_timestamps[0] - 5 #seconds <-- offset um früher zu beginnen (die Timestamps sind eigentlich in us aber  mit kommazahl.. deswegen sekunde)\n",
    "        ending = sender_timestamps[len(sender_timestamps)-1] + 30 #seconds\n",
    "        all_beginnings.append(beginning) # Array aller Timestamp files Starts - 5 Sekunden\n",
    "        all_endings.append(ending) # Array aller Timestamp files Endes + 30 Sekunden\n",
    "    print(all_beginnings)\n",
    "    print(all_endings)\n",
    "\n",
    "    current_sender = 0\n",
    "    num_of_senders = len(timestamps_of_all_senders) # Das macht keinen sinn für mich.. \n",
    "    #Sind nicht die anzahl der Sender sondern die Anzahl der Timestamp files! Und die sind einfach nur unterteilt (fortlaufende timestamps und IDs)\n",
    "    current_sender_received_packets = []\n",
    "    flag = 0\n",
    "    for p in packets:\n",
    "        if p.get('timestamp') > all_beginnings[current_sender] and p.get('timestamp') < all_endings[current_sender]:\n",
    "            current_sender_received_packets.append(p) # Wenn Packet innerhalb des Zeitbereichs des Timestamp files ist (hier als Sender) wird es zu den empfangenen Paketen hinzugefügt\n",
    "\n",
    "        elif current_sender == num_of_senders - 1: # Wenn am ende der Timestamp files (hier sender)\n",
    "            packets_of_all_receivers.append(current_sender_received_packets) # Füge das Array der pakete (dieses Timestamp files) dem Gesammtarray hinzu\n",
    "            break\n",
    "        elif p.get('timestamp') >= all_beginnings[current_sender + 1] and p.get('timestamp') < all_endings[current_sender + 1]: # Wenn Packet innerhalb des Zeitbereichs des nächsten Timestamp files liegt (hier sender)\n",
    "            packets_of_all_receivers.append(current_sender_received_packets)\n",
    "            current_sender += 1 # Wähle das nächste Timestamp file aus (hier sender)\n",
    "            current_sender_received_packets = []\n",
    "            current_sender_received_packets.append(p)\n",
    "    return packets_of_all_receivers\n",
    "\n",
    "def separate_receiver_packets_chronological_traces(packets, timestamps_of_all_senders):\n",
    "    \"\"\"\n",
    "    Same as non Chronological ? @see separate_receiver_packets_non_chronological_traces\n",
    "    \"\"\"\n",
    "    packets_of_all_receivers = []\n",
    "    all_beginnings = []\n",
    "    all_endings = []\n",
    "    for sender_timestamps in timestamps_of_all_senders:\n",
    "        beginning = sender_timestamps[0] - 5 #seconds\n",
    "        ending = sender_timestamps[len(sender_timestamps)-1] + 30 #seconds\n",
    "        all_beginnings.append(beginning)\n",
    "        all_endings.append(ending)\n",
    "    print(all_beginnings)\n",
    "    print(all_endings)\n",
    "    current_sender = 0\n",
    "    num_of_senders = len(timestamps_of_all_senders)\n",
    "    current_sender_received_packets = []\n",
    "    for p in packets:            \n",
    "        if p.get('timestamp') > all_beginnings[current_sender] and p.get('timestamp') < all_endings[current_sender]:\n",
    "            current_sender_received_packets.append(p)\n",
    "        elif p.get('timestamp') >= all_endings[current_sender]:\n",
    "            packets_of_all_receivers.append(current_sender_received_packets)\n",
    "            if current_sender < num_of_senders - 1:\n",
    "                current_sender += 1\n",
    "                current_sender_received_packets = []\n",
    "                if p.get('timestamp') > all_beginnings[current_sender] and p.get('timestamp') < all_endings[current_sender]:\n",
    "                    current_sender_received_packets.append(p)\n",
    "            else:\n",
    "                break\n",
    "    return packets_of_all_receivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "packets_of_all_receivers = separate_receiver_packets_non_chronological_traces(packets, timestamps_of_all_senders)\n",
    "packets_towards_receivers = []\n",
    "\"\"\"\n",
    "Contain Arrays with the Packets grouped by the Timestamp files\n",
    "\"\"\"\n",
    "packets_towards_receivers = [[p for p in packets if (p.get('to_amazon') is False)] for packets in packets_of_all_receivers] # Ultra hässlich.. -> Selectiert alle packete die \"to Amazon\" = false sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time(timestamps, start_time):\n",
    "    \"\"\"\n",
    "    Converts timestamps to relative timestamps (relative to Startime)\n",
    "    \"\"\"\n",
    "    converted_timestamps = []\n",
    "    for t in timestamps:\n",
    "        converted = t - start_time\n",
    "        if converted < 0:\n",
    "            print('Error: timestamp is larger than start time with t = {} and start_time = {}'.format(t, start_time))\n",
    "        converted_timestamps.append(converted)\n",
    "    return converted_timestamps    \n",
    "\n",
    "def get_on_periods_of_sender_2(timestamps, message_sizes):\n",
    "    \"\"\"\n",
    "    Returns an Array of timestamp and messagesSizes Tupels\\\\\n",
    "    eg.   \n",
    "     `[(timestamps[0], messagesizes[0]), (timestamps[1], messagesizes[1]).... ]`\n",
    "    \"\"\"\n",
    "    global TransitionPeriod # Macht doch gar nichts ... können wir auch weglöschen?\n",
    "    bursts = []\n",
    "    timestamps_Sizes_tupel = zip(timestamps, message_sizes) # Creates a Tupel... (timestamps[0], messagesizes[0])...\n",
    "    for t, s in timestamps_Sizes_tupel:\n",
    "        bursts.append((t, s))\n",
    "    return bursts\n",
    "\n",
    "\"\"\" def get_on_periods_of_user(timestamps, user_packet_sizes): # Wird nicht benutzt\n",
    "    global TransitionPeriod\n",
    "    is_in_burst = False\n",
    "    bursts = []\n",
    "    burst_size = 0\n",
    "    last_burst_index = 0\n",
    "    for time_iter in range(0, len(timestamps)):\n",
    "        if time_iter == len(timestamps) - 1 and is_in_burst == True:\n",
    "            if burst_size > 1514:\n",
    "                bursts.append((timestamps[last_burst_index], burst_size))\n",
    "                break\n",
    "        if timestamps[time_iter] - timestamps[last_burst_index] > TransitionPeriod:\n",
    "            if is_in_burst == True:\n",
    "                if burst_size > 1514:\n",
    "                    bursts.append((timestamps[last_burst_index], burst_size))\n",
    "                is_in_burst = False\n",
    "                burst_size = 0\n",
    "        if user_packet_sizes[time_iter] > 1400:\n",
    "            if is_in_burst == False:\n",
    "                is_in_burst = True\n",
    "                last_burst_index = time_iter\n",
    "            burst_size += user_packet_sizes[time_iter]\n",
    "    return bursts\n",
    "\n",
    "\n",
    "def get_bursts_user_2(timestamps, user_packet_sizes): # Wird nicht benutzt\n",
    "    bursts = []\n",
    "    burst_size = 0\n",
    "    for time_iter in range(0, len(timestamps) - 1):\n",
    "        ipd = timestamps[time_iter + 1] - timestamps[time_iter]\n",
    "        if ipd < 1:\n",
    "            if user_packet_sizes[time_iter] > 40:\n",
    "                burst_size += user_packet_size[time_iter]\n",
    "        else:\n",
    "            if burst_size > 20000:\n",
    "                bursts.append((timestamps[time_iter], burst_size))\n",
    "            burst_size = 0\n",
    "    return bursts \"\"\"\n",
    "\n",
    "def get_bursts_user(timestamps, user_packet_sizes):\n",
    "    \"\"\"\n",
    "    Returnes A List of Tupels containing the Timestamps and the Packet Burstsizes (Filters only Correct burst (40bytes threshold))\n",
    "    Merges Packets together that belong to one Burst (one Second ?) some Magic constant -> Should be the T_e Parameter\n",
    "    \"\"\"\n",
    "    global TransitionPeriod\n",
    "    is_in_burst = False\n",
    "    bursts = []\n",
    "    burst_size = 0\n",
    "    last_burst_index = 0\n",
    "    MTU = 1500 #TODO Constant ist damit wirklich die MTU gemeint ? ... Oder wird damit nur nochmal doppelt geprüft falls leitung mit geringer bandbreite verwendet wird.. Aber dann macht MTU size keinen sinn..\n",
    "\n",
    "    for time_iter in range(0, len(timestamps)): # Time_iter ist index für jede Nachricht (markiert durch timestamp)\n",
    "        if time_iter == len(timestamps) - 1 and is_in_burst: # Wenn letzte Msg und wir innerhalb eines Bursts sind ?\n",
    "            if burst_size > MTU: # Und es ein Burst Paket ist -> Verstehe den MTU parameter nicht so richtig -> Seite 8 unten rechts\n",
    "                bursts.append((timestamps[last_burst_index], burst_size)) # Speichern wir diesen Burst, so wie auch normal (Nur das hier richtigerweise der LastburstIndex richtig ist)!!!\n",
    "                break\n",
    "\n",
    "        if (timestamps[time_iter] - timestamps[last_burst_index]) > 1: # Wenn mehr als eine Sekunde vergangen ist seit ende des letzten Burst -> Sind wir nicht mehr in einem Burst\n",
    "            if is_in_burst:\n",
    "                if burst_size > MTU:\n",
    "                    #TODO is Programmier Fehler hier ? müsste nicht last_burst_index sein? Siehe seite 9 und 6 Zeilen weiter oben\n",
    "                    bursts.append((timestamps[time_iter], burst_size)) # Speichern des Bursts (als Tupel Timestamp mit der Burstgröße) (Warum nicht den Letzten Burst Timestamp???!!)\n",
    "                is_in_burst = False #-> Sind wir nicht mehr in einem Burst\n",
    "                burst_size = 0\n",
    "        if user_packet_sizes[time_iter] > MAGIC_IsUserPacketARelevantBurstPacket: # magic constant -> Wenn packet size > 40 Bytes (Wahrscheinlich wenn nicht nur steuerungsinformationen) #TODO\n",
    "            if is_in_burst == False:\n",
    "                is_in_burst = True # Befinden wir uns in einem Burst\n",
    "            last_burst_index = time_iter # Setzen des des Bursts (wird immer neu gesetzt -> Markiert das Ende)\n",
    "            burst_size += user_packet_sizes[time_iter] # Addieren des Bursts\n",
    "    return bursts\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"def merge_channel_period_points(ChannelPeriodPoints):\n",
    "    \n",
    "    #Wird das überhaupt benutzt ?\n",
    "    \n",
    "    #Merge Entrys together that are closer then 2 [unit missing? Seconds?]\n",
    "    #Outputs List of Tupels\n",
    "\n",
    "    #-> Könnte sein um das mergen durch die Event Extraction (get_bursts_user) nachzubilden..\n",
    "    #-> Hat aber einen anderen Threshold..\n",
    "    \n",
    "    c = []\n",
    "    time = [x[0] for x in ChannelPeriodPoints] # Schreibt alle times ChannelPeriodPoints[i].getTime() in time\n",
    "    size = [x[1] for x in ChannelPeriodPoints] # Schreibt alle sizes ChannelPeriodPoints[i].getSize() in Size\n",
    "    burst_time = []\n",
    "    burst_volume = []\n",
    "    burst_volume.append(size[0])\n",
    "    burst_time.append(time[0])\n",
    "    temp = time[0]\n",
    "    for i in range(len(time)-1): # for(int i = 0; i < time.length(); i++)\n",
    "        if (time[i+1] - temp) < 2: # Wenn abstand zum letzten kleiner als 2 ?? #TODO Magic Constant\n",
    "            burst_volume[-1] += size[i+1] # [-1] is the last element (wrap around) | Addiert auf das letzte burst_volume die nächste Größe\n",
    "            temp = time[i+1]\n",
    "        else:\n",
    "            burst_time.append(time[i+1]) # Und sonst wenn ausserhalb des bereichs.. Füge weiteres Time ELement hinzu\n",
    "            burst_volume.append(size[i+1]) # Und weitere größe\n",
    "            temp = time[i+1]\n",
    "    for i in range(len(burst_time)):\n",
    "        c.append((burst_time[i], burst_volume[i]))\n",
    "    return c\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"def get_channel_points(f): # Wird nicht benutzt\n",
    "    ChannelPeriodPoints = []\n",
    "    first_line = True\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        if first_line:\n",
    "            find_start_time(line[1].decode('ascii'))\n",
    "            first_line = False\n",
    "            continue\n",
    "        message_type = line[4].decode('ascii')\n",
    "        if message_type == 'text':\n",
    "            continue\n",
    "        time = convert_time(line[2].decode('ascii'))\n",
    "        if line[5].decode('ascii') == 'None':\n",
    "            size = 300000\n",
    "        else:\n",
    "            size = int(line[5].decode('ascii'))\n",
    "        ChannelPeriodPoints.append((time, size))\n",
    "    if len(ChannelPeriodPoints) == 0:\n",
    "        return []\n",
    "    c = merge_channel_period_points(ChannelPeriodPoints)\n",
    "    return c\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hier wird extraction gemacht glaube ich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'packets_towards_receivers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-429fec97924e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mall_channel_bursts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# the same as adversary bursts in the case of one-on-one chats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpackets_towards_receivers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Zählt durch alle Timestamp Gruppen (die Arrays mit den Paketen enthalten)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Hour {} {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage_file_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Warum auch immer das Hour sind ?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'packets_towards_receivers' is not defined"
     ]
    }
   ],
   "source": [
    "all_user_bursts = []\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "all_channel_bursts = [] # the same as adversary bursts in the case of one-on-one chats\n",
    "\n",
    "for i in range(0, len(packets_towards_receivers)): # Zählt durch alle Timestamp Gruppen (die Arrays mit den Paketen enthalten)  \n",
    "\n",
    "    print('Hour {} {}'.format(i, message_file_names[i])) # Warum auch immer das Hour sind ?\n",
    "\n",
    "    channel_timestamps = timestamps_of_all_senders[i] # Hier wird entpackt... die Timestamps der Nachrichten die Mitbeobachtet wurden im Channel (durch Adversary messaging Bot)\n",
    "    channel_message_sizes = message_sizes_of_all_senders[i]\n",
    "\n",
    "    user_packet_timestamps = [p.get('timestamp') for p in packets_towards_receivers[i]] # Alle Timestamps extrahieren in dem konkkreten Timestamp file\n",
    "    user_packet_sizes = [p.get('size') for p in packets_towards_receivers[i]] # Alle größen extrahieren\n",
    "\n",
    "    if len(user_packet_timestamps) == 0: # Wenn nichts enthält wird weitergegangen\n",
    "        all_user_bursts.append([])\n",
    "        all_channel_bursts.append([])\n",
    "        print('no receiver timestamps')\n",
    "        continue\n",
    "    \n",
    "    start_time = min(channel_timestamps[0], user_packet_timestamps[0]) # Channel timestamps kommen aus dem Timestamp file - User Packet timestamps aus den Packets .. der start wird ermittelt\n",
    "    print(channel_timestamps[0])\n",
    "    print(user_packet_timestamps[0])\n",
    "    print('start time: {} diff: {}'.format(start_time,abs(channel_timestamps-user_packet_timestamps)))\n",
    "\n",
    "    converted_sender_timestamps = convert_time(channel_timestamps, start_time) # Sets all timestamps (from timestamp file) relativ to the first of this file\n",
    "    sender_period_points = get_on_periods_of_sender_2(converted_sender_timestamps, channel_message_sizes) # Creates Tupel list with message sizes and Converted Timestamps\n",
    "\n",
    "    # Does the same with the Timestamps from the Captured Packets\n",
    "    converted_user_timestamps = convert_time(user_packet_timestamps, start_time)\n",
    "    # And gets the Analyzed Bursts with the Timestamps (relativ to the start of this page)\n",
    "    user_period_points = get_bursts_user(converted_user_timestamps, user_packet_sizes)\n",
    "\n",
    "    all_user_bursts.append(user_period_points)\n",
    "    all_channel_bursts.append(sender_period_points)\n",
    "    print ('Hour {} is done'.format(i))\n",
    "    print (\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the Bursts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stores the Bursts in Pickles \n",
    "\n",
    "with open(os.path.join(results_dir, 'all_user_bursts.pickle'), 'wb') as handle:\n",
    "    pickle.dump(all_user_bursts, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(os.path.join(results_dir, 'all_channel_bursts.pickle'), 'wb') as handle:\n",
    "    pickle.dump(all_channel_bursts, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(results_dir, 'all_user_bursts.pickle'), 'rb') as handle:\n",
    "    all_user_bursts = pickle.load(handle)\n",
    "with open(os.path.join(results_dir, 'all_channel_bursts.pickle'), 'rb') as handle:\n",
    "    all_channel_bursts = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intervals(period_points, minute):\n",
    "    \"\"\"\n",
    "    Splits the Period Points list into Time Intervals. Specified by \"minute\"\n",
    "    \n",
    "    `return:` list of periodPoints (wahrscheinlich msgEvents). Jedes Listenelement ist eine Liste mit msgEvents innerhalb eines Intervals\n",
    "    \"\"\"\n",
    "    global LENGTH #=3600\n",
    "    num_of_intervals = int(LENGTH / minute)\n",
    "    intervals = []\n",
    "    for i in range(num_of_intervals):\n",
    "        intervals.append([]) # the Empty element\n",
    "        startpoint = i * minute\n",
    "        endpoint = (i + 1) * minute\n",
    "        for j in range(len(period_points)):\n",
    "            if startpoint <= period_points[j][0] and period_points[j][0] < endpoint:\n",
    "                intervals[-1].append(period_points[j])\n",
    "    return intervals\n",
    "\n",
    "def find_intervals_channel(period_points, minute, message_types):\n",
    "    \"\"\"\n",
    "    same as find_intervals. Except that it filters out text type Messages.. \n",
    "    copy and Paste Code\n",
    "    \"\"\"\n",
    "    global LENGTH #=3600 = Eine stunde! \n",
    "    num_of_intervals = int(LENGTH / minute) # Wieviele Intervalle innerhalb einer Stunde. Weil die Period Points pro Stunde sind.. \n",
    "    intervals = []\n",
    "    for i in range(num_of_intervals):\n",
    "        intervals.append([])\n",
    "        startpoint = i * minute\n",
    "        endpoint = (i + 1) * minute\n",
    "        for j in range(len(period_points)):\n",
    "            if startpoint <= period_points[j][0] and period_points[j][0] < endpoint and message_types[j] != 'text':\n",
    "                intervals[-1].append(period_points[j])\n",
    "    return intervals\n",
    "\n",
    "def find_matches(channel_interval, user_interval):\n",
    "    \"\"\"\n",
    "    Bekommt einen Award für die Hässlichste Lösung bisher\n",
    "\n",
    "    Vergleicht channel Intervals mit den Userintervals, zählt alle Matches der Events (time und size)\n",
    "    Ein Match ist wenn die time (max) +- DELTA sekunden auseinanderliegt, und die Größen abweichung kleiner ist als GAMMA*Size (Also die relative größe des Bursts)\n",
    "\n",
    "    `Return:` Die Treffer Quote (number_of_matches / all Events)\n",
    "    \"\"\"\n",
    "    number_of_matches = 0\n",
    "    number_of_nonmatches = 0\n",
    "    matched_user_intervals = set()\n",
    "    # Vergleicht jedes Event aus channel_interval mit jedem user_intervall (Versucht diese zu machten. Mit Gamma (Size Threshold) und Delta (Time Threshold))\n",
    "    # Probiert verschiedene Deltas durch (Zeitliche Abweichungen)\n",
    "    # Wenn ein Match gefunden wurde, dann wird mit dem nächsten channel_interval fortgeführt\n",
    "    for j in range(min(len(channel_interval), len(user_interval))):\n",
    "        time = channel_interval[j][0]\n",
    "        size = channel_interval[j][1]\n",
    "        is_matched = False\n",
    "        for d in range(1, DELTA): # Innerhalb von 1-15 sekunden -> Geht verschiedene Delta werte durch. Zeitliche Differenz. Um uhrzeit drifft auszugleichen\n",
    "            for i in range(len(user_interval)):\n",
    "                if user_interval[i] in matched_user_intervals: # Wenn  schon gematcht -> Weitergehen\n",
    "                    continue\n",
    "                if abs(user_interval[i][0] - time) < d: # Wenn innerhalb des Deltas (welches schrittweise erhöt wird) Also Differenz der beiden Timestamps\n",
    "                    if abs(size - user_interval[i][1]) < GAMMA * size: # Hmm hier wird gamma in Abhändigkei der Größe gesetzt. Relative Abweichung... (Im Paper anders )\n",
    "                        matched_user_intervals.add(user_interval[i])\n",
    "                        number_of_matches += 1\n",
    "                        is_matched = True\n",
    "                        break\n",
    "            else:\n",
    "                continue # Wird gemacht wenn kein Match gefunden wurde (also die schleife nicht mit Break verlassen wurde)\n",
    "            break # Wird gemacht wenn ein Match gefunden wurde..\n",
    "        if is_matched is False:\n",
    "            number_of_nonmatches += 1\n",
    "    return number_of_matches / float(number_of_matches + number_of_nonmatches)\n",
    "\n",
    "def detect_correlations_from_intervals(channel_intervals, user_intervals):\n",
    "    \"\"\"\n",
    "    `Return` liste mit Treffer Quote für jedes Interval.\\\\\n",
    "        `-1` wenn keine Events in einem Interval\\\\\n",
    "        `0` wenn nur ein Event in `channel_interval` bzw `user_interval`\n",
    "\n",
    "    \"\"\"\n",
    "    correlation = []\n",
    "    for i in range(len(channel_intervals)): # Was ist wenn einer der beiden weniger oder mehr Elemente hat ?! o.O\n",
    "        if len(channel_intervals[i]) == 0 and len(user_intervals[i]) == 0: # no events in either one\n",
    "            correlation.append(-1)\n",
    "        elif len(channel_intervals[i]) == 0 or len(user_intervals[i]) == 0: # only one has an event\n",
    "            correlation.append(0)\n",
    "        else:\n",
    "            correlation.append(find_matches(channel_intervals[i], user_intervals[i]))\n",
    "    return correlation\n",
    "\n",
    "\n",
    "def detect_correlations_from_bursts(channel_bursts, user_bursts, message_types, j): #Input Data is one Dataset file (one File -> One Hour)\n",
    "    \"\"\"\n",
    "    1. Splits the Bursts into Observation Intervals (Observation Lengths are provided by `j` arg)\\\\\n",
    "    2. Detects Matches\\\\\n",
    "    `Return`: a Match Ratios for every Interval (as List/Array)\n",
    "    \"\"\"\n",
    "    global observation_lengths\n",
    "    correlations = []\n",
    "    user_intervals = find_intervals(user_bursts, observation_lengths[j])\n",
    "    channel_intervals = find_intervals_channel(channel_bursts, observation_lengths[j], message_types) # Wird wahrscheinlich benutzt weil text messages eh zu klein sind..\n",
    "    correlation_of_intervals = detect_correlations_from_intervals(channel_intervals, user_intervals)\n",
    "    return correlation_of_intervals\n",
    "\n",
    "\n",
    "def remove_empty_correlations(correlations): # Not Used\n",
    "    organized = []\n",
    "    for i in range(len(correlations)):\n",
    "        organized.append([c for c in correlations[i] if c != -1])\n",
    "    return organized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate correlations for positive samples (correlated pairs of flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_correlations_tp = [] # Ist dann eine Liste von Matchratio List pro File\n",
    "for j in range(len(observation_lengths)): # Test Correlation_Match_Ratios for Different Obsveration Lengths\n",
    "    print('For {}-second interval:'.format(observation_lengths[j]))\n",
    "    corrs_tp = []\n",
    "    for i in range(len(all_user_bursts)): # for every hour (every File?)\n",
    "        corrs = detect_correlations_from_bursts(all_channel_bursts[i], all_user_bursts[i], message_types_of_all_senders[i], j)\n",
    "        if corrs != -1:\n",
    "            corrs_tp.extend(corrs)\n",
    "        print (\"Hour {} is done\".format(i))\n",
    "    all_correlations_tp.append([c for c in corrs_tp if c != -1]) # MAAAAN die haben sich doch extra eine Funktion geschrieben die alle negativen herausfiltert.. Stattdessen machen die hier so eine häßliche scheiße\n",
    "with open(os.path.join(results_dir, 'corrs_tp_event_based_delta_{}.pickle'.format(DELTA)), 'wb') as handle: # Abspeichern der Match Ratios\n",
    "    pickle.dump(all_correlations_tp, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate correlations for negative samples (uncorrelated pairs of flows):\n",
    "\n",
    "Geht alle Pages durch (``all_user_bursts``, ``all_channel_bursts``).. und versucht diese miteinander zu matchen. \\\n",
    "bei gleichen Indizes wird geskippt. \\\n",
    "Dadurch wird versucht events zu matchen die keine sind. -> Dadurch FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate correlations for false samples (uncorrelated pairs of flows)\n",
    "\n",
    "all_corrs_fp = []\n",
    "for m in range(len(observation_lengths)):\n",
    "    print('For {}-second interval:'.format(observation_lengths[m]))\n",
    "    \n",
    "    corrs_fp = []\n",
    "#     for j in range(len(all_user_bursts[i])):\n",
    "    for i in range(len(all_user_bursts)):\n",
    "        for j in range(len(all_channel_bursts)):\n",
    "            if i == j:\n",
    "                continue # Skippt wenn gleicher Index (Also Korrelierende Files sind)\n",
    "            corrs = detect_correlations_from_bursts(all_channel_bursts[j], all_user_bursts[i], message_types_of_all_senders[j], m)\n",
    "            if len(corrs) == 0:\n",
    "                print('warning')\n",
    "    #             continue\n",
    "#             organized_correlations = remove_empty_correlations(corrs)\n",
    "    #         for k in range(len(organized_correlations)):\n",
    "    #             corrs_fp[k].extend(organized_correlations[k])\n",
    "            if corrs != -1:\n",
    "                corrs_fp.extend(corrs)\n",
    "            print (\"user {} with channel {} is done\".format(i, j))\n",
    "    all_corrs_fp.append([c for c in corrs_fp if c != -1])\n",
    "with open(os.path.join(results_dir, 'corrs_fp_event_based_delta_{}.pickle'.format(DELTA)), 'wb') as handle:\n",
    "    pickle.dump(all_corrs_fp, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "436d27cdadab617090263f3acec30a147bd9c61037429ea03c70f22899a6ef48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
